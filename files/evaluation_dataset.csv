,question,contexts,ground_truth,evolution_type,metadata,episode_done
0,How are node embeddings updated in a simple Graph CNN layer?,"['250\n13\nGraph neural networks\nFigure 13.7 Simple Graph CNN layer. a) Input graph consists of structure (em-\nbodied in graph adjacency matrix A, not shown) and node embeddings (stored\nin columns of X). b) Each node in the first hidden layer is updated by (i) ag-\ngregating the neighboring nodes to form a single vector, (ii) applying a linear\ntransformation Ω0 to the aggregated nodes, (iii) applying the same linear trans-\nformation Ω0 to the original node, (iv) adding these together with a bias β0,\nand finally (v) applying a nonlinear activation function a[•] like a ReLU. c) This\nprocess is repeated at subsequent layers (but with different parameters for each\nlayer) until we produce the final embeddings at the end of the network.\nfrom a node that is “above” the node of interest differently to information from a node\nthat is “below” it.\n13.4.3\nExample GCN layer\nThese considerations lead to a simple GCN layer (figure 13.7). At each node n in layer k,\nwe aggregate information from neighboring nodes by summing their node embeddings h•:\nagg[n, k] =\nX\nm∈ne[n]\nh(m)\nk\n,\n(13.8)\nwhere ne[n] returns the set of indices of the neighbors of node n.\nThen we apply a\nlinear transformation Ωk to the embedding h(n)\nk\nat the current node and to this ag-\ngregated value, add a bias term βk, and pass the result through a nonlinear activation\nfunction a[•], which is applied independently to every member of its vector argument:\nh(n)\nk+1 = a\nh\nβk + Ωk · h(n)\nk\n+ Ωk · agg[n, k]\ni\n.\n(13.9)\nWe can write this more succinctly by noting that post-multiplication of a matrix\nby a vector returns a weighted sum of its columns. The nth column of the adjacency\nmatrix A contains ones at the positions of the neighbors. Hence, if we collect the node\nThis work is subject to a Creative Commons CC-BY-NC-ND license. (C) MIT Press.\n']","In a simple Graph CNN layer, node embeddings are updated by (i) aggregating the neighboring nodes to form a single vector, (ii) applying a linear transformation to the aggregated nodes, (iii) applying the same linear transformation to the original node, (iv) adding these together with a bias, and finally (v) applying a nonlinear activation function.",simple,"[{'source': './files/UnderstandingDeepLearning_08_05_24_C.pdf', 'file_path': './files/UnderstandingDeepLearning_08_05_24_C.pdf', 'page': 263, 'total_pages': 541, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': 'D:20240805224447Z', 'modDate': ""D:20240805184627-04'00'"", 'trapped': ''}]",True
1,"What is an adjacency matrix in graph representation, and how is it defined?","['13.2\nGraph representation\n243\nFigure 13.3 Graph representation. a) Example graph with six nodes and seven\nedges. Each node has an associated embedding of length five (brown vectors).\nEach edge has an associated embedding of length four (blue vectors). This graph\ncan be represented by three matrices. b) The adjacency matrix is a binary matrix\nwhere element (m, n) is set to one if node m connects to node n. c) The node\ndata matrix X contains the concatenated node embeddings. d) The edge data\nmatrix E contains the edge embeddings.\nThe point set representing the airplane in figure 13.2d can be converted into a graph\nby connecting each point to its K nearest neighbors. The result is a geometric graph\nwhere each point is associated with a position in 3D space. Figure 13.2e represents a\nhierarchical graph. The table, light, and room are each described by graphs representing\nthe adjacency of their respective components. These three graphs are themselves nodes\nin another graph that represents the topology of the objects in a larger model.\nAll types of graphs can be processed using deep learning.\nHowever, this chapter\nfocuses on undirected graphs like the social network in figure 13.2a.\n13.2\nGraph representation\nIn addition to the graph structure itself, information is typically associated with each\nnode. For example, in a social network, each individual might be characterized by a fixed-\nlength vector representing their interests. Sometimes, the edges also have information\nattached. For example, in the road network example, each edge might be characterized\nby its length, number of lanes, frequency of accidents, and speed limit. The information\nat a node is stored in a node embedding, and the information at an edge is stored in an\nedge embedding.\nMore formally, a graph consists of a set of N nodes connected by a set of E edges. The\ngraph can be encoded by three matrices A, X, and E, representing the graph structure,\nnode embeddings, and edge embeddings, respectively (figure 13.3).\nDraft: please send errata to udlbookmail@gmail.com.\n']","The adjacency matrix is a binary matrix where element (m, n) is set to one if node m connects to node n.",simple,"[{'source': './files/UnderstandingDeepLearning_08_05_24_C.pdf', 'file_path': './files/UnderstandingDeepLearning_08_05_24_C.pdf', 'page': 256, 'total_pages': 541, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': 'D:20240805224447Z', 'modDate': ""D:20240805184627-04'00'"", 'trapped': ''}]",True
2,What is the relationship between the least squares criterion and the fit of a linear model?,"['5.3\nExample 1: univariate regression\n63\nFigure 5.4 Equivalence of least squares and maximum likelihood loss for the\nnormal distribution.\na) Consider the linear model from figure 2.2.\nThe least\nsquares criterion minimizes the sum of the squares of the deviations (dashed lines)\nbetween the model prediction f[xi, ϕ] (green line) and the true output values yi\n(orange points). Here the fit is good, so these deviations are small (e.g., for the\ntwo highlighted points). b) For these parameters, the fit is bad, and the squared\ndeviations are large. c) The least squares criterion follows from the assumption\nthat the model predicts the mean of a normal distribution over the outputs and\nthat we maximize the probability. For the first case, the model fits well, so the\nprobability Pr(yi|xi) of the data (horizontal orange dashed lines) is large (and\nthe negative log probability is small). d) For the second case, the model fits badly,\nso the probability is small and the negative log probability is large.\nDraft: please send errata to udlbookmail@gmail.com.\n']","The least squares criterion minimizes the sum of the squares of the deviations between the model prediction and the true output values. A good fit results in small deviations, while a bad fit results in large deviations. This criterion follows from the assumption that the model predicts the mean of a normal distribution over the outputs and that we maximize the probability.",simple,"[{'source': './files/UnderstandingDeepLearning_08_05_24_C.pdf', 'file_path': './files/UnderstandingDeepLearning_08_05_24_C.pdf', 'page': 76, 'total_pages': 541, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': 'D:20240805224447Z', 'modDate': ""D:20240805184627-04'00'"", 'trapped': ''}]",True
3,What are the desiderata for network layers in normalizing flows?,"['308\n16\nNormalizing flows\n\x0c\n\x0c\n\x0c\n\x0c\n∂f[z, ϕ]\n∂z\n\x0c\n\x0c\n\x0c\n\x0c =\n\x0c\n\x0c\n\x0c\n\x0c\n∂fK[fK−1, ϕK]\n∂fK−1\n\x0c\n\x0c\n\x0c\n\x0c ·\n\x0c\n\x0c\n\x0c\n\x0c\n∂fK−1[fK−2, ϕK−1]\n∂fK−2\n\x0c\n\x0c\n\x0c\n\x0c . . .\n\x0c\n\x0c\n\x0c\n\x0c\n∂f2[f1, ϕ2]\n∂f1\n\x0c\n\x0c\n\x0c\n\x0c ·\n\x0c\n\x0c\n\x0c\n\x0c\n∂f1[z, ϕ1]\n∂z\n\x0c\n\x0c\n\x0c\n\x0c .\n(16.7)\nThe absolute determinant of the Jacobian of the inverse mapping is found by applying\nProblem 16.3\nthe same rule to equation 16.5. It is the reciprocal of the absolute determinant in the\nforward mapping.\nWe train normalizing flows with a dataset {xi} of I training examples using the\nnegative log-likelihood criterion:\nˆ\nϕ\n=\nargmax\nϕ\n"" I\nY\ni=1\nPr(zi) ·\n\x0c\n\x0c\n\x0c\n\x0c\n∂f[zi, ϕ]\n∂zi\n\x0c\n\x0c\n\x0c\n\x0c\n−1#\n=\nargmin\nϕ\n"" I\nX\ni=1\nlog\n""\x0c\n\x0c\n\x0c\n\x0c\n∂f[zi, ϕ]\n∂zi\n\x0c\n\x0c\n\x0c\n\x0c\n#\n−log\n\x02\nPr(zi)\n\x03\n#\n,\n(16.8)\nwhere zi = f−1[xi, ϕ], Pr(zi) is measured under the base distribution, and the absolute\ndeterminant |∂f[zi, ϕ]/∂zi| is given by equation 16.7.\n16.2.2\nDesiderata for network layers\nThe theory of normalizing flows is straightforward. However, for this to be practical, we\nneed neural network layers fk that have four properties.\n1. Collectively, the set of network layers must be suﬀiciently expressive to map a\nmultivariate standard normal distribution to an arbitrary density.\n2. The network layers must be invertible; each must define a unique one-to-one map-\nping from any input point to an output point (a bijection). If multiple inputs were\nAppendix B.1\nBijection\nmapped to the same output, the inverse would be ambiguous.\n3. It must be possible to compute the inverse of each layer eﬀiciently.\nWe need\nto do this every time we evaluate the likelihood. This happens repeatedly during\ntraining, so there must be a closed-form solution or a fast algorithm for the inverse.\n4. It also must be possible to evaluate the determinant of the Jacobian eﬀiciently for\neither the forward or inverse mapping.\n16.3\nInvertible network layers\nWe now describe different invertible network layers or flows for use in these models.\nWe start with linear and elementwise flows. These are easy to invert, and it’s possible\nto compute the determinant of their Jacobians, but neither is suﬀiciently expressive to\ndescribe arbitrary transformations of the base density. However, they form the building\nblocks of coupling, autoregressive, and residual flows, which are all more expressive.\nThis work is subject to a Creative Commons CC-BY-NC-ND license. (C) MIT Press.\n']","The desiderata for network layers in normalizing flows are: 1) the set of network layers must be sufficiently expressive to map a multivariate standard normal distribution to an arbitrary density, 2) the network layers must be invertible and define a unique one-to-one mapping from any input point to an output point, 3) it must be possible to compute the inverse of each layer efficiently, and 4) it must be possible to evaluate the determinant of the Jacobian efficiently for either the forward or inverse mapping.",simple,"[{'source': './files/UnderstandingDeepLearning_08_05_24_C.pdf', 'file_path': './files/UnderstandingDeepLearning_08_05_24_C.pdf', 'page': 321, 'total_pages': 541, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': 'D:20240805224447Z', 'modDate': ""D:20240805184627-04'00'"", 'trapped': ''}]",True
4,What is the role of the diffusion kernel q(zt|x) in the diffusion encoder process?,"['18.2\nEncoder (forward process)\n353\npoint (figure 18.4). It can be computed by considering the joint distribution q(x, z1...t)\nAppendix C.1.2\nMarginalization\nand marginalizing over all the variables except zt:\nq(zt)\n=\nZ Z\nq(z1...t, x)dz1...t−1dx\n=\nZ Z\nq(z1...t|x)Pr(x)dz1...t−1dx,\n(18.9)\nwhere q(z1...t|x) was defined in equation 18.3.\nHowever, since we now have an expression for the diffusion kernel q(zt|x) that “skips”\nthe intervening variables, we can equivalently write:\nq(zt) =\nZ\nq(zt|x)Pr(x)dx.\n(18.10)\nHence, if we repeatedly sample from the data distribution Pr(x) and superimpose the\ndiffusion kernel q(zt|x) on each sample, the result is the marginal distribution q(zt) (fig-\nNotebook 18.1\nDiffusion encoder\nure 18.4). However, the marginal distribution cannot be written in closed form because\nwe don’t know the original data distribution Pr(x).\n18.2.3\nConditional distribution q(zt−1|zt)\nWe defined the conditional probability q(zt|zt−1) as the mixing process (equation 18.2).\nAppendix C.1.4\nBayes’ rule\nTo reverse this process, we apply Bayes’ rule:\nq(zt−1|zt) = q(zt|zt−1)q(zt−1)\nq(zt)\n.\n(18.11)\nThis is intractable since we cannot compute the marginal distribution q(zt−1).\nFor this simple 1D example, it’s possible to evaluate q(zt−1|zt) numerically (fig-\nure 18.5). In general, their form is complex, but in many cases, they are well-approximated\nby a normal distribution. This is important because when we build the decoder, we will\napproximate the reverse process using a normal distribution.\n18.2.4\nConditional diffusion distribution q(zt−1|zt, x)\nThere is one final distribution related to the encoder to consider. We noted above that\nwe could not find the conditional distribution q(zt−1|zt) because we do not know the\nmarginal distribution q(zt−1). However, if we know the starting variable x, then we\ndo know the distribution q(zt−1|x) at the time before. This is just the diffusion kernel\n(figure 18.3), and it is normally distributed.\nHence, it is possible to compute the conditional diffusion distribution q(zt−1|zt, x)\nin closed form (figure 18.6). This distribution is used to train the decoder. It is the\ndistribution over zt−1 when we know the current latent variable zt and the training\nDraft: please send errata to udlbookmail@gmail.com.\n']","The diffusion kernel q(zt|x) plays a crucial role in the diffusion encoder process as it allows us to compute the marginal distribution q(zt) by superimposing the kernel on each sample from the data distribution Pr(x). This kernel ""skips"" the intervening variables, enabling us to write q(zt) as the integral of q(zt|x)Pr(x)dx.",simple,"[{'source': './files/UnderstandingDeepLearning_08_05_24_C.pdf', 'file_path': './files/UnderstandingDeepLearning_08_05_24_C.pdf', 'page': 366, 'total_pages': 541, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': 'D:20240805224447Z', 'modDate': ""D:20240805184627-04'00'"", 'trapped': ''}]",True
5,Backpropagation method for neural nets?,"['100\n7\nGradients and initialization\nAs we move backward through the network, we see that most of the terms we need\nwere already calculated in the previous step, so we do not need to re-compute them.\nProceeding backward through the network in this way to compute the derivatives is\nknown as the backward pass.\nThe ideas behind backpropagation are relatively easy to understand. However, the\nderivation requires matrix calculus because the bias and weight terms are vectors and\nmatrices, respectively. To help grasp the underlying mechanics, the following section\nderives backpropagation for a simpler toy model with scalar parameters. We then apply\nthe same approach to a deep neural network in section 7.4.\n7.3\nToy example\nConsider a model f[x, ϕ] with eight scalar parameters ϕ = {β0, ω0, β1, ω1, β2, ω2, β3, ω3}\nthat consists of a composition of the functions sin[•], exp[•], and cos[•]:\nf[x, ϕ] = β3 + ω3 · cos\nh\nβ2 + ω2 · exp\n\x02\nβ1 + ω1 · sin[β0 + ω0 · x]\n\x03i\n,\n(7.5)\nand a least squares loss function L[ϕ] = P\ni ℓi with individual terms:\nℓi = (f[xi, ϕ] −yi)2,\n(7.6)\nwhere, as usual, xi is the ith training input, and yi is the ith training output. You can\nthink of this as a simple neural network with one input, one output, one hidden unit at\neach layer, and different activation functions sin[•], exp[•], and cos[•] between each layer.\nWe aim to compute the derivatives:\n∂ℓi\n∂β0\n,\n∂ℓi\n∂ω0\n,\n∂ℓi\n∂β1\n,\n∂ℓi\n∂ω1\n,\n∂ℓi\n∂β2\n,\n∂ℓi\n∂ω2\n,\n∂ℓi\n∂β3\n,\nand\n∂ℓi\n∂ω3\n.\n(7.7)\nOf course, we could find expressions for these derivatives by hand and compute them\ndirectly. However, some of these expressions are quite complex. For example:\n∂ℓi\n∂ω0\n=\n−2\n\x10\nβ3 + ω3 · cos\nh\nβ2 + ω2 · exp\n\x02\nβ1 + ω1 · sin[β0 + ω0 · xi]\n\x03i\n−yi\n\x11\n·ω1ω2ω3 · xi · cos[β0 + ω0 · xi] · exp\nh\nβ1 + ω1 · sin[β0 + ω0 · xi]\ni\n· sin\n\x14\nβ2 + ω2 · exp\nh\nβ1 + ω1 · sin[β0 + ω0 · xi]\ni\x15\n.\n(7.8)\nSuch expressions are awkward to derive and code without mistakes and do not exploit\nthe inherent redundancy; notice that the three exponential terms are the same.\nThe backpropagation algorithm is an eﬀicient method for computing all of these\nderivatives at once. It consists of (i) a forward pass, in which we compute and store a\nseries of intermediate values and the network output, and (ii) a backward pass, in which\nThis work is subject to a Creative Commons CC-BY-NC-ND license. (C) MIT Press.\n']","The backpropagation algorithm is an efficient method for computing the derivatives of the loss function with respect to the model parameters. It consists of a forward pass, in which we compute and store a series of intermediate values and the network output, and a backward pass, in which we compute the derivatives of the loss function with respect to the model parameters.",reasoning,"[{'source': './files/UnderstandingDeepLearning_08_05_24_C.pdf', 'file_path': './files/UnderstandingDeepLearning_08_05_24_C.pdf', 'page': 113, 'total_pages': 541, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': 'D:20240805224447Z', 'modDate': ""D:20240805184627-04'00'"", 'trapped': ''}]",True
6,Loss functions for graph regression & binary classification?,"['246\n13\nGraph neural networks\nFigure 13.5 Permutation of node indices. a) Example graph, b) associated adja-\ncency matrix and c) node embeddings. d) The same graph where the (arbitrary)\norder of the indices has been changed.\ne) The adjacency matrix and f) node\nmatrix are now different. Consequently, any network layer that operates on the\ngraph should be indifferent to the ordering of the nodes.\n13.3.1\nTasks and loss functions\nWe defer discussion of graph neural network models until section 13.4 and first describe\nthe types of problems these networks tackle and their associated loss functions. Super-\nvised graph problems usually fall into one of three categories (figure 13.6).\nGraph-level tasks:\nThe network assigns a label or estimates one or more values from\nthe entire graph, exploiting both the structure and node embeddings. For example, we\nmight want to predict the temperature at which a molecule becomes liquid (a regression\ntask) or whether a molecule is poisonous to human beings or not (a classification task).\nFor graph-level tasks, the output node embeddings are combined (e.g., by averaging),\nand the resulting vector is mapped via a linear transformation or neural network to a\nfixed-size vector. For regression, the mismatch between the result and the ground truth\nvalues is computed using the least squares loss. For binary classification, the output\nis passed through a sigmoid function, and the mismatch is calculated using the binary\ncross-entropy loss. Here, the probability that the graph belongs to class one might be\ngiven by:\nPr(y = 1|X, A) = sig [βK + ωKHK1/N] ,\n(13.2)\nwhere the scalar βK and 1 × D vector ωK are learned parameters. Post-multiplying the\noutput embedding matrix HK by the column vector 1 that contains ones has the effect\nof summing together all the embeddings and subsequently dividing by the number of\nnodes N computes the average. This is known as mean pooling (see figure 10.11).\nThis work is subject to a Creative Commons CC-BY-NC-ND license. (C) MIT Press.\n']","For graph-level tasks, the loss functions used are the least squares loss for regression and the binary cross-entropy loss for binary classification.",reasoning,"[{'source': './files/UnderstandingDeepLearning_08_05_24_C.pdf', 'file_path': './files/UnderstandingDeepLearning_08_05_24_C.pdf', 'page': 259, 'total_pages': 541, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': 'D:20240805224447Z', 'modDate': ""D:20240805184627-04'00'"", 'trapped': ''}]",True
7,How can a 2-hidden-layer network be broken down into 2 shallow ones?,"['4.2\nFrom composing networks to deep networks\n43\nh′\n1\n=\na[θ′\n10 + θ′\n11y]\nh′\n2\n=\na[θ′\n20 + θ′\n21y]\nh′\n3\n=\na[θ′\n30 + θ′\n31y],\n(4.3)\nand\ny′ = ϕ′\n0 + ϕ′\n1h′\n1 + ϕ′\n2h′\n2 + ϕ′\n3h′\n3.\n(4.4)\nWith ReLU activations, this model also describes a family of piecewise linear functions.\nHowever, the number of linear regions is potentially greater than for a shallow network\nwith six hidden units. To see this, consider choosing the first network to produce three\nProblem 4.1\nalternating regions of positive and negative slope (figure 4.1b). This means that three\ndifferent ranges of x are mapped to the same output range y ∈[−1, 1], and the subsequent\nmapping from this range of y to y′ is applied three times. The overall effect is that the\nNotebook 4.1\nComposing\nnetworks\nfunction defined by the second network is duplicated three times to create nine linear\nregions. The same principle applies in higher dimensions (figure 4.2).\nA different way to think about composing networks is that the first network “folds”\nthe input space x back onto itself so that multiple inputs generate the same output.\nThen the second network applies a function, which is replicated at all points that were\nfolded on top of one another (figure 4.3).\n4.2\nFrom composing networks to deep networks\nThe previous section showed that we could create complex functions by passing the\noutput of one shallow neural network into a second network. We now show that this is\na special case of a deep network with two hidden layers.\nThe output of the first network (y = ϕ0 + ϕ1h1 + ϕ2h2 + ϕ3h3) is a linear combina-\ntion of the activations at the hidden units. The first operations of the second network\n(equation 4.3 in which we calculate θ′\n10 + θ′\n11y, θ′\n20 + θ′\n21y, and θ′\n30 + θ′\n31y) are linear in\nthe output of the first network. Applying one linear function to another yields another\nlinear function. Substituting the expression for y into equation 4.3 gives:\nh′\n1\n=\na[θ′\n10 + θ′\n11y]\n=\na[θ′\n10 + θ′\n11ϕ0 + θ′\n11ϕ1h1 + θ′\n11ϕ2h2 + θ′\n11ϕ3h3]\nh′\n2\n=\na[θ′\n20 + θ′\n21y]\n=\na[θ′\n20 + θ′\n21ϕ0 + θ′\n21ϕ1h1 + θ′\n21ϕ2h2 + θ′\n21ϕ3h3]\nh′\n3\n=\na[θ′\n30 + θ′\n31y]\n=\na[θ′\n30 + θ′\n31ϕ0 + θ′\n31ϕ1h1 + θ′\n31ϕ2h2 + θ′\n31ϕ3h3], (4.5)\nwhich we can rewrite as:\nh′\n1\n=\na[ψ10 + ψ11h1 + ψ12h2 + ψ13h3]\nh′\n2\n=\na[ψ20 + ψ21h1 + ψ22h2 + ψ23h3]\nh′\n3\n=\na[ψ30 + ψ31h1 + ψ32h2 + ψ33h3],\n(4.6)\nDraft: please send errata to udlbookmail@gmail.com.\n']","A 2-hidden-layer network can be broken down into 2 shallow ones by considering the first network as producing an output that is a linear combination of the activations at the hidden units, and then the second network applies a function to this output. This can be seen as a special case of a deep network with two hidden layers, where the output of the first network is passed into the second network.",multi_context,"[{'source': './files/UnderstandingDeepLearning_08_05_24_C.pdf', 'file_path': './files/UnderstandingDeepLearning_08_05_24_C.pdf', 'page': 56, 'total_pages': 541, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': 'D:20240805224447Z', 'modDate': ""D:20240805184627-04'00'"", 'trapped': ''}]",True
8,What is the role of a linear transformation in a simple Graph CNN layer?,"['250\n13\nGraph neural networks\nFigure 13.7 Simple Graph CNN layer. a) Input graph consists of structure (em-\nbodied in graph adjacency matrix A, not shown) and node embeddings (stored\nin columns of X). b) Each node in the first hidden layer is updated by (i) ag-\ngregating the neighboring nodes to form a single vector, (ii) applying a linear\ntransformation Ω0 to the aggregated nodes, (iii) applying the same linear trans-\nformation Ω0 to the original node, (iv) adding these together with a bias β0,\nand finally (v) applying a nonlinear activation function a[•] like a ReLU. c) This\nprocess is repeated at subsequent layers (but with different parameters for each\nlayer) until we produce the final embeddings at the end of the network.\nfrom a node that is “above” the node of interest differently to information from a node\nthat is “below” it.\n13.4.3\nExample GCN layer\nThese considerations lead to a simple GCN layer (figure 13.7). At each node n in layer k,\nwe aggregate information from neighboring nodes by summing their node embeddings h•:\nagg[n, k] =\nX\nm∈ne[n]\nh(m)\nk\n,\n(13.8)\nwhere ne[n] returns the set of indices of the neighbors of node n.\nThen we apply a\nlinear transformation Ωk to the embedding h(n)\nk\nat the current node and to this ag-\ngregated value, add a bias term βk, and pass the result through a nonlinear activation\nfunction a[•], which is applied independently to every member of its vector argument:\nh(n)\nk+1 = a\nh\nβk + Ωk · h(n)\nk\n+ Ωk · agg[n, k]\ni\n.\n(13.9)\nWe can write this more succinctly by noting that post-multiplication of a matrix\nby a vector returns a weighted sum of its columns. The nth column of the adjacency\nmatrix A contains ones at the positions of the neighbors. Hence, if we collect the node\nThis work is subject to a Creative Commons CC-BY-NC-ND license. (C) MIT Press.\n']","In a simple Graph CNN layer, a linear transformation is applied to the aggregated neighboring nodes and to the original node, and the results are added together with a bias term before passing through a nonlinear activation function.",simple,"[{'source': './files/UnderstandingDeepLearning_08_05_24_C.pdf', 'file_path': './files/UnderstandingDeepLearning_08_05_24_C.pdf', 'page': 263, 'total_pages': 541, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': 'D:20240805224447Z', 'modDate': ""D:20240805184627-04'00'"", 'trapped': ''}]",True
