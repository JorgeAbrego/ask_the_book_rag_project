{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from langchain_postgres import PGVector\n",
    "from langchain_postgres.vectorstores import PGVector\n",
    "load_dotenv()\n",
    "PG_VECTOR_PWD = os.environ[\"PG_VECTOR_PWD\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/llmprj_1/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/home/ubuntu/miniconda3/envs/llmprj_1/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_embedding = HuggingFaceEmbeddings(model_name='multi-qa-mpnet-base-dot-v1')\n",
    "\n",
    "connection = f\"postgresql+psycopg://vector_user:{PG_VECTOR_PWD}@localhost:5431/vector_db\"\n",
    "collection_name = \"udlbook\"\n",
    "\n",
    "vector_store = PGVector(\n",
    "    embeddings=model_embedding,\n",
    "    collection_name=collection_name,\n",
    "    connection=connection,\n",
    "    use_jsonb=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyMuPDFLoader(\"../files/UnderstandingDeepLearning_08_05_24_C.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'UnderstandingDeepLearning_08_05_24_C.pdf', 'file_path': 'UnderstandingDeepLearning_08_05_24_C.pdf', 'page': 0, 'total_pages': 541, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': 'D:20240805224447Z', 'modDate': \"D:20240805184627-04'00'\", 'trapped': ''}, page_content='Understanding Deep Learning\\nSimon J.D. Prince\\nAugust 5, 2024\\nIf you enjoy this book, here are four ways you can help me:\\n1. Spread the word via social media. Posts in languages other than English par-\\nticularly welcome. Tag me on LinkedIn or X and I’ll probably say hi.\\n2. Write me an Amazon review.\\nPreferably positive, but all publicity is good\\npublicity...\\n3. Send me comments (see bottom of this page). I reply to everything eventually.\\n4. Buy a copy. I took 18 months completely off work to write this book and ideally\\nI’d like to make minimum wage (or better) for this time. Also, I’d like to write\\na second edition, but I need to sell enough copies to do this. Thanks!\\nThe most recent version of this document can be found at http://udlbook.com.\\nCopyright in this work has been licensed exclusively to The MIT Press,\\nhttps://mitpress.mit.edu, which released the final version to the public in December 2023.\\nInquiries regarding rights should be addressed to the MIT Press, Rights & Permissions\\nDepartment.\\nThis work is subject to a Creative Commons CC-BY-NC-ND license.\\nI would really appreciate help improving this document. No detail too small! Please contact\\nme with suggestions, factual inaccuracies, ambiguities, questions, and errata via github or by\\ne-mail at udlbookmail@gmail.com.\\n'),\n",
       " Document(metadata={'source': 'UnderstandingDeepLearning_08_05_24_C.pdf', 'file_path': 'UnderstandingDeepLearning_08_05_24_C.pdf', 'page': 1, 'total_pages': 541, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': 'D:20240805224447Z', 'modDate': \"D:20240805184627-04'00'\", 'trapped': ''}, page_content=''),\n",
       " Document(metadata={'source': 'UnderstandingDeepLearning_08_05_24_C.pdf', 'file_path': 'UnderstandingDeepLearning_08_05_24_C.pdf', 'page': 2, 'total_pages': 541, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': 'D:20240805224447Z', 'modDate': \"D:20240805184627-04'00'\", 'trapped': ''}, page_content='This book is dedicated to Blair, Calvert, Coppola, Ellison, Faulkner, Kerpatenko, Morris,\\nRobinson, Sträussler, Wallace, Waymon, Wojnarowicz, and all the others whose work is\\neven more important and interesting than deep learning.\\n')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='38\n",
      "3\n",
      "Shallow neural networks\n",
      "functions, which saturate (become close to zero) for large positive and large negative inputs.\n",
      "However, the ReLU function has the disadvantage that its derivative is zero for negative inputs.\n",
      "If all the training examples produce negative inputs to a given ReLU function, then we cannot\n",
      "improve the parameters feeding into this ReLU during training. The gradient with respect to\n",
      "the incoming weights is locally flat, so we cannot “walk downhill.” This is known as the dying\n",
      "ReLU problem.\n",
      "Many variations on the ReLU have been proposed to resolve this problem\n",
      "(figure 3.13b), including (i) the leaky ReLU (Maas et al., 2013), which also has a linear output\n",
      "for negative values with a smaller slope of 0.1, (ii) the parametric ReLU (He et al., 2015), which\n",
      "treats the slope of the negative portion as an unknown parameter, and (iii) the concatenated\n",
      "ReLU (Shang et al., 2016), which produces two outputs, one of which clips below zero (i.e., like\n",
      "a typical ReLU) and one of which clips above zero.\n",
      "A variety of smooth functions have also been investigated (figure 3.13c–d), including the soft-\n",
      "plus function (Glorot et al., 2011), Gaussian error linear unit (Hendrycks & Gimpel, 2016),\n",
      "sigmoid linear unit (Hendrycks & Gimpel, 2016), and exponential linear unit (Clevert et al.,\n",
      "2015). Most of these are attempts to avoid the dying ReLU problem while limiting the gradient\n",
      "for negative values. Klambauer et al. (2017) introduced the scaled exponential linear unit (fig-\n",
      "ure 3.13e), which is particularly interesting as it helps stabilize the variance of the activations\n",
      "when the input variance has a limited range (see section 7.5). Ramachandran et al. (2017)\n",
      "adopted an empirical approach to choosing an activation function. They searched the space\n",
      "of possible functions to find the one that performed best over a variety of supervised learning\n",
      "tasks. The optimal function was found to be a[x] = x/(1 + exp[−βx]), where β is a learned\n",
      "parameter (figure 3.13f). They termed this function Swish. Interestingly, this was a rediscovery\n",
      "of activation functions previously proposed by Hendrycks & Gimpel (2016) and Elfwing et al.\n",
      "(2018). Howard et al. (2019) approximated Swish by the HardSwish function, which has a very\n",
      "similar shape but is faster to compute:\n",
      "HardSwish[z] =\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0\n",
      "z < −3\n",
      "z(z + 3)/6\n",
      "−3 ≤z ≤3\n",
      "z\n",
      "z > 3\n",
      ".\n",
      "(3.13)\n",
      "There is no definitive answer as to which of these activations functions is empirically superior.\n",
      "However, the leaky ReLU, parameterized ReLU, and many of the continuous functions can be\n",
      "shown to provide minor performance gains over the ReLU in particular situations. We restrict\n",
      "attention to neural networks with the basic ReLU function for the rest of this book because it’s\n",
      "easy to characterize the functions they create in terms of the number of linear regions.\n",
      "Universal approximation theorem:\n",
      "The width version of this theorem states that there\n",
      "exists a network with one hidden layer containing a finite number of hidden units that can\n",
      "approximate any specified continuous function on a compact subset of Rn to arbitrary accuracy.\n",
      "This was proved by Cybenko (1989) for a class of sigmoid activations and was later shown to\n",
      "be true for a larger class of nonlinear activation functions (Hornik, 1991).\n",
      "Number of linear regions:\n",
      "Consider a shallow network with Di ≥2-dimensional inputs\n",
      "and D hidden units. The number of linear regions is determined by the intersections of the D\n",
      "hyperplanes created by the “joints” in the ReLU functions (e.g., figure 3.8d–f). Each region is\n",
      "created by a different combination of the ReLU functions clipping or not clipping the input.\n",
      "Appendix B.2\n",
      "Binomial\n",
      "coeﬀicient\n",
      "The number of regions created by D hyperplanes in the Di ≤D-dimensional input space was\n",
      "Problem 3.18\n",
      "shown by Zaslavsky (1975) to be at most PDi\n",
      "j=0\n",
      "\u0000D\n",
      "j\n",
      "\u0001\n",
      "(i.e., a sum of binomial coeﬀicients). As a\n",
      "rule of thumb, shallow neural networks almost always have a larger number D of hidden units\n",
      "than input dimensions Di and create between 2Di and 2D linear regions.\n",
      "This work is subject to a Creative Commons CC-BY-NC-ND license. (C) MIT Press.\n",
      "' metadata={'source': 'UnderstandingDeepLearning_08_05_24_C.pdf', 'file_path': 'UnderstandingDeepLearning_08_05_24_C.pdf', 'page': 51, 'total_pages': 541, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'xdvipdfmx (20200315)', 'creationDate': 'D:20240805224447Z', 'modDate': \"D:20240805184627-04'00'\", 'trapped': ''}\n"
     ]
    }
   ],
   "source": [
    "print(data[51])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "541"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents =data[500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    return text.replace('\\x00', '')\n",
    "\n",
    "for doc in documents:\n",
    "    doc.page_content = clean_text(doc.page_content)\n",
    "    # If there are any other text fields, clean them as well\n",
    "    if \"metadata\" in doc:\n",
    "        for key in doc.metadata:\n",
    "            if isinstance(doc.metadata[key], str):\n",
    "                doc.metadata[key] = clean_text(doc.metadata[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[500,\n",
       " 501,\n",
       " 502,\n",
       " 503,\n",
       " 504,\n",
       " 505,\n",
       " 506,\n",
       " 507,\n",
       " 508,\n",
       " 509,\n",
       " 510,\n",
       " 511,\n",
       " 512,\n",
       " 513,\n",
       " 514,\n",
       " 515,\n",
       " 516,\n",
       " 517,\n",
       " 518,\n",
       " 519,\n",
       " 520,\n",
       " 521,\n",
       " 522,\n",
       " 523,\n",
       " 524,\n",
       " 525,\n",
       " 526,\n",
       " 527,\n",
       " 528,\n",
       " 529,\n",
       " 530,\n",
       " 531,\n",
       " 532,\n",
       " 533,\n",
       " 534,\n",
       " 535,\n",
       " 536,\n",
       " 537,\n",
       " 538,\n",
       " 539,\n",
       " 540]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.add_documents(documents, ids=[doc.metadata[\"page\"] for doc in documents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Document(metadata={'page': 15, 'title': '', 'author': '', 'format': 'PDF 1.5', 'source': 'UnderstandingDeepLearning_08_05_24_C.pdf', 'creator': 'LaTeX with hyperref', 'modDate': \"D:20240805184627-04'00'\", 'subject': '', 'trapped': '', 'keywords': '', 'producer': 'xdvipdfmx (20200315)', 'file_path': 'UnderstandingDeepLearning_08_05_24_C.pdf', 'total_pages': 541, 'creationDate': 'D:20240805224447Z'}, page_content='2\\n1\\nIntroduction\\nFigure 1.1 Machine learning is an area\\nof artificial intelligence that fits math-\\nematical models to observed data.\\nIt\\ncan coarsely be divided into supervised\\nlearning, unsupervised learning, and re-\\ninforcement learning. Deep neural net-\\nworks contribute to each of these areas.\\n1.1.1\\nRegression and classification problems\\nFigure 1.2 depicts several regression and classification problems. In each case, there is a\\nmeaningful real-world input (a sentence, a sound file, an image, etc.), and this is encoded\\nas a vector of numbers. This vector forms the model input. The model maps the input to\\nan output vector which is then “translated” back to a meaningful real-world prediction.\\nFor now, we focus on the inputs and outputs and treat the model as a black box that\\ningests a vector of numbers and returns another vector of numbers.\\nThe model in figure 1.2a predicts the price of a house based on input characteristics\\nsuch as the square footage and the number of bedrooms. This is a regression problem\\nbecause the model returns a continuous number (rather than a category assignment).\\nIn contrast, the model in figure 1.2b takes the chemical structure of a molecule as an\\ninput and predicts both the freezing and boiling points. This is a multivariate regression\\nproblem since it predicts more than one number.\\nThe model in figure 1.2c receives a text string containing a restaurant review as input\\nand predicts whether the review is positive or negative. This is a binary classification\\nproblem because the model attempts to assign the input to one of two categories. The\\noutput vector contains the probabilities that the input belongs to each category. Fig-\\nures 1.2d and 1.2e depict multiclass classification problems. Here, the model assigns the\\ninput to one of N > 2 categories. In the first case, the input is an audio file, and the\\nmodel predicts which genre of music it contains. In the second case, the input is an\\nimage, and the model predicts which object it contains. In each case, the model returns\\na vector of size N that contains the probabilities of the N categories.\\n1.1.2\\nInputs\\nThe input data in figure 1.2 varies widely. In the house pricing example, the input is a\\nfixed-length vector containing values that characterize the property. This is an example\\nof tabular data because it has no internal structure; if we change the order of the inputs\\nand build a new model, then we expect the model prediction to remain the same.\\nConversely, the input in the restaurant review example is a body of text. This may\\nbe of variable length depending on the number of words in the review, and here input\\nThis work is subject to a Creative Commons CC-BY-NC-ND license. (C) MIT Press.\\n'), 0.360274462323918)\n",
      "\n",
      "(Document(metadata={'page': 14, 'title': '', 'author': '', 'format': 'PDF 1.5', 'source': 'UnderstandingDeepLearning_08_05_24_C.pdf', 'creator': 'LaTeX with hyperref', 'modDate': \"D:20240805184627-04'00'\", 'subject': '', 'trapped': '', 'keywords': '', 'producer': 'xdvipdfmx (20200315)', 'file_path': 'UnderstandingDeepLearning_08_05_24_C.pdf', 'total_pages': 541, 'creationDate': 'D:20240805224447Z'}, page_content='Chapter 1\\nIntroduction\\nArtificial intelligence, or AI, is concerned with building systems that simulate intelligent\\nbehavior. It encompasses a wide range of approaches, including those based on logic,\\nsearch, and probabilistic reasoning. Machine learning is a subset of AI that learns to\\nmake decisions by fitting mathematical models to observed data. This area has seen\\nexplosive growth and is now (incorrectly) almost synonymous with the term AI.\\nA deep neural network (or deep network for short) is a type of machine learning\\nmodel, and when it is fitted to data, this is referred to as deep learning. At the time of\\nwriting, deep networks are the most powerful and practical machine learning models and\\nare often encountered in day-to-day life. It is commonplace to translate text to another\\nlanguage using a natural language processing algorithm, to search for images of a given\\nobject using a computer vision system, or to converse with a digital assistant via a speech\\nrecognition interface. All of these applications are powered by deep learning.\\nAs the title suggests, this book aims to help a reader new to this field understand\\nthe principles behind deep learning. The book is neither terribly theoretical (there are\\nno proofs) nor extremely practical (there is almost no code). The goal is to explain the\\nunderlying ideas; after consuming this volume, the reader will be able to apply deep\\nlearning to novel situations where there is no existing recipe for success.\\nMachine learning methods can coarsely be divided into three areas: supervised, unsu-\\npervised, and reinforcement learning. At the time of writing, the cutting-edge methods\\nin all three areas rely on deep learning (figure 1.1). This introductory chapter describes\\nthese three areas at a high level, and this taxonomy is also loosely reflected in the book’s\\norganization. Whether we like it or not, deep learning is poised to change our world,\\nand this change will not all be positive. Hence, this chapter also contains a brief primer\\non AI ethics. We conclude with advice on how to make the most of this book.\\n1.1\\nSupervised learning\\nSupervised learning models define a mapping from input data to an output prediction.\\nIn the following sections, we discuss the inputs, the outputs, the model itself, and what\\nis meant by “training” a model.\\nDraft: please send errata to udlbookmail@gmail.com.\\n'), 0.3767849983655851)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"What is Machine Learning?\"\n",
    "similar = vector_store.similarity_search_with_score(query, k=2)\n",
    "\n",
    "for doc in similar:\n",
    "    print(doc, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmprj_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
