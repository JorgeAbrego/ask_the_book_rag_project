{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_postgres import PGVector\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "sys.path.append('../..')\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "PG_VECTOR_PWD = os.environ[\"PG_VECTOR_PWD\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/llmprj_1/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/home/ubuntu/miniconda3/envs/llmprj_1/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_embedding = HuggingFaceEmbeddings(model_name='multi-qa-mpnet-base-dot-v1')\n",
    "\n",
    "connection = f\"postgresql+psycopg://vector_user:{PG_VECTOR_PWD}@localhost:5431/vector_db\"\n",
    "collection_name = \"udlbook\"\n",
    "\n",
    "vector_store = PGVector(\n",
    "    embeddings=model_embedding,\n",
    "    collection_name=collection_name,\n",
    "    connection=connection,\n",
    "    use_jsonb=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(search_kwargs={'k': 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_llama3_1 = ChatGroq(\n",
    "    model=\"llama-3.1-70b-versatile\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")\n",
    "\n",
    "llm_gemma2 = ChatGroq(\n",
    "    model=\"gemma2-9b-it\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")\n",
    "\n",
    "llm_mixtral = ChatGroq(\n",
    "    model=\"mixtral-8x7b-32768\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"You are an expert in the book 'Understanding Deep Learning'. \n",
    "Use the following pieces of retrieved context to answer the question.\n",
    "If you don't know the answer, just say that you don't know.\n",
    "Keep the answer upto 5 lines unless the user asks for more information\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "qa_rag_chain_llama3_1 = (\n",
    "{\n",
    "\"context\": (retriever\n",
    "|\n",
    "format_docs),\n",
    "\"question\": RunnablePassthrough()\n",
    "}\n",
    "|\n",
    "prompt_template\n",
    "|\n",
    "llm_llama3_1\n",
    ")\n",
    "\n",
    "qa_rag_chain_gemma2 = (\n",
    "{\n",
    "\"context\": (retriever\n",
    "|\n",
    "format_docs),\n",
    "\"question\": RunnablePassthrough()\n",
    "}\n",
    "|\n",
    "prompt_template\n",
    "|\n",
    "llm_gemma2\n",
    ")\n",
    "\n",
    "qa_rag_chain_mixtral = (\n",
    "{\n",
    "\"context\": (retriever\n",
    "|\n",
    "format_docs),\n",
    "\"question\": RunnablePassthrough()\n",
    "}\n",
    "|\n",
    "prompt_template\n",
    "|\n",
    "llm_mixtral\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine learning is a subset of AI that learns to make decisions by fitting mathematical models to observed data.\n",
      "content='Machine learning is a subset of AI that learns to make decisions by fitting mathematical models to observed data.' response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 704, 'total_tokens': 725, 'completion_time': 0.084, 'prompt_time': 0.165740096, 'queue_time': 0.005318881999999997, 'total_time': 0.249740096}, 'model_name': 'llama-3.1-70b-versatile', 'system_fingerprint': 'fp_b3ae7e594e', 'finish_reason': 'stop', 'logprobs': None} id='run-a1742dc8-0c35-479f-b239-a5a10d7e1831-0' usage_metadata={'input_tokens': 704, 'output_tokens': 21, 'total_tokens': 725}\n"
     ]
    }
   ],
   "source": [
    "query = 'What is machine learning'\n",
    "result_l = qa_rag_chain_llama3_1.invoke(query)\n",
    "print(result_l.content)\n",
    "print(result_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_g = qa_rag_chain_gemma2.invoke(query)\n",
    "print(result_g.content)\n",
    "print(result_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_m = qa_rag_chain_mixtral.invoke(query)\n",
    "print(result_m.content)\n",
    "print(result_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'What is the difference between machine learning and deep learning?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_l = qa_rag_chain_llama3_1.invoke(query)\n",
    "print(result_l.content)\n",
    "print(result_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_g = qa_rag_chain_gemma2.invoke(query)\n",
    "print(result_g.content)\n",
    "print(result_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_m = qa_rag_chain_mixtral.invoke(query)\n",
    "print(result_m.content)\n",
    "print(result_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'Why does deep learning work? Please be detailed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_l = qa_rag_chain_llama3_1.invoke(query)\n",
    "print(result_l.content)\n",
    "print(result_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_g = qa_rag_chain_gemma2.invoke(query)\n",
    "print(result_g.content)\n",
    "print(result_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_m = qa_rag_chain_mixtral.invoke(query)\n",
    "print(result_m.content)\n",
    "print(result_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'What are transformers and what is the difference with convolutional network?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_l = qa_rag_chain_llama3_1.invoke(query)\n",
    "print(result_l.content)\n",
    "print(result_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_g = qa_rag_chain_gemma2.invoke(query)\n",
    "print(result_g.content)\n",
    "print(result_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_m = qa_rag_chain_mixtral.invoke(query)\n",
    "print(result_m.content)\n",
    "print(result_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmprj_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
